{
 "metadata": {
  "name": "",
  "signature": "sha256:1f4b7040e97c66e7fc97ddaf29498b1560991952187e34bb0ebb79d0f965b2bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pandas as pd\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn import tree, dummy, metrics\n",
      "import statsmodels.formula.api as smf\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Overview\n",
      "\n",
      "The general workflow for this project is:\n",
      "1. Create a cleanup function that handles both the in and out of sample data (**clean**)\n",
      "2. Create a model function that returns an sklearn classifier based on an X and y (**model**)\n",
      "3. Use **clean** to generate an X and y and pass them to **model**\n",
      "4. Pass the resulting classifier, X, and y to to the **test_model** helper to see how it does\n",
      "5. Submit the results by using the **create_submission** with the classifier and **clean** on the out of sample set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Helper Functions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Project Specific"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_model(X, y, clf, bench=0.5):\n",
      "    \"\"\"\n",
      "    Tests the ROC AUC of a number of dummies and the benchmark against a model score, and returns the results\n",
      "    X          : the independent variable matrix\n",
      "    y          : the target variable\n",
      "    clf        : the sklearn classifier\n",
      "    bench      : benchmark AUC score to test against (default is 0.5)\n",
      "    \"\"\"\n",
      "    model_score = metrics.roc_auc_score(y, clf.predict(X))\n",
      "    scores = [['model', model_score],['benchmark', bench]]\n",
      "    for strat in ['stratified', 'most_frequent', 'uniform']:\n",
      "        dummyclf = dummy.DummyClassifier(strategy=strat).fit(X, y)\n",
      "        scores.append([strat, metrics.roc_auc_score(y, dummyclf.predict(X))])\n",
      "    \n",
      "    # print results\n",
      "    print \"TEST MODEL AUC: %0.4f\" % scores[0][1]\n",
      "    print \"======================\"\n",
      "    for i in scores[1:5]:\n",
      "        print '%s: %0.4f (%s)' % (i[0], i[1], i[1] < model_score)\n",
      "    \n",
      "    return scores\n",
      "\n",
      "\n",
      "def create_submission(X, clf, name='submission.csv'):\n",
      "    \"\"\"\n",
      "    Creates a submission file based on an sklearn classifier and out of sample data set\n",
      "    X       : the data set (MUST CONTAIN RefId)\n",
      "    clf     : the sklearn classifier\n",
      "    lem_oos : the out of sample data set\n",
      "    name    : the name of the submissions file (defaults to 'submissions.csv')\n",
      "    \"\"\"\n",
      "    y_pred = clf.predict(X.drop(labels=['RefId'], axis=1))\n",
      "    submission = pd.DataFrame({'RefId': X.RefId, 'prediction' : y_pred })\n",
      "    submission.to_csv('submissions/' + name)\n",
      "    print \"Test file \\'%s\\' successfully created.\" % name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def basic_metrics(y_true, y_predict):\n",
      "    \"\"\"\n",
      "    prints the accuracy, confusion matrix and ROC AUC score for two boolean arrays of the same size\n",
      "    \"\"\"\n",
      "    print \"Accuracy: %0.4f\" % (metrics.accuracy_score(y_true, y_predict))\n",
      "    print \"ROC AUC:  %0.4f\" % (metrics.roc_auc_score(y_true, y_predict))\n",
      "    print \"Confusion Matrix:\"\n",
      "    print metrics.confusion_matrix(y_true, y_predict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Data Import and cleaning\n",
      "\n",
      "In order to generate the output for the submission, the out of sample data must be cleaned the same way and have the same new columns created from the data. In order to do this the following helper functions have been created, so that different iterations of data cleaning and variable generation can be maintained.\n",
      "\n",
      "The main difference between cleaning the 'oos' data and the training dataa is that the training data needs to maintain **IsBadBuy** as a target variable, while the 'oos' data needs to maintain **RefId**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import the lemons\n",
      "lemons = pd.read_csv('data/lemons.csv')\n",
      "lemons_oos = pd.read_csv('data/lemons_oos.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def benchmark_lemons(lem, oos=False):\n",
      "    # Generating a list of continuous data features from the describe dataframe. \n",
      "    # Then, removing the two non-features (RefId is an index, IsBadBuy is the prediction value)\n",
      "    ldf = lem.dropna(axis=1)\n",
      "    features = list(ldf.describe().columns)\n",
      "    if not oos:\n",
      "        features.remove('RefId')\n",
      "    \n",
      "    return ldf[features]\n",
      "\n",
      "def clean_lemons_a(lem, oos=False):\n",
      "    \"\"\"\n",
      "    Returns cleaned up lemons data set with new variables and dummies added\n",
      "    lem    : the data set\n",
      "    oos    : if True, ignore the target_cols (default is Fals)\n",
      "    \"\"\"\n",
      "    # columns\n",
      "    cont_cols = ['VehYear', 'VehicleAge', u'VehOdo', u'MMRAcquisitionAuctionAveragePrice',\n",
      "             u'MMRAcquisitionAuctionCleanPrice', u'MMRAcquisitionRetailAveragePrice',\n",
      "             u'MMRAcquisitonRetailCleanPrice', u'MMRCurrentAuctionAveragePrice', u'MMRCurrentAuctionCleanPrice',\n",
      "             u'MMRCurrentRetailAveragePrice', u'MMRCurrentRetailCleanPrice', u'VehBCost', u'WarrantyCost']\n",
      "    dummy_cols = ['Make','Transmission','Size','AUCGUART', 'Auction']\n",
      "    target_col = ['IsBadBuy']\n",
      "    if oos:\n",
      "        target_col = ['RefId']\n",
      "\n",
      "    # create the data frame used for analysis\n",
      "    ldf = lem[target_col + cont_cols + dummy_cols]\n",
      "\n",
      "    # correct the null values in AUCGART\n",
      "    ldf.AUCGUART = ldf.AUCGUART.map({'GREEN':1 ,'RED':0, nan: 0})\n",
      "\n",
      "    # make Transmission a dummy variable Auto = 0, Manual = 1\n",
      "    ldf.Transmission = ldf.Transmission.map({'AUTO':0, 'MANUAL':1, 'Manual':1})\n",
      "\n",
      "    #drop rows with missing data\n",
      "    ldf = ldf.dropna()\n",
      "\n",
      "    # create a number of dummy variables based on categorical features and drop the original columns\n",
      "    ldf = ldf.join(pd.get_dummies(ldf['Make'], prefix='make'))\n",
      "    ldf = ldf.join(pd.get_dummies(ldf['Size'], prefix='size'))\n",
      "    ldf = ldf.join(pd.get_dummies(ldf['Auction'], prefix='auction'))\n",
      "    ldf = ldf.drop(labels=['Make', 'Size', 'Auction'], axis=1)\n",
      "                       \n",
      "    return ldf\n",
      "\n",
      "def clean_lemons_b(lem, oos=False):\n",
      "    \"\"\"same as clean_lemons_a, but adds back in color\"\"\"\n",
      "    \n",
      "    # columns\n",
      "    cont_cols = ['VehYear', 'VehicleAge', u'VehOdo', u'MMRAcquisitionAuctionAveragePrice',\n",
      "             u'MMRAcquisitionAuctionCleanPrice', u'MMRAcquisitionRetailAveragePrice',\n",
      "             u'MMRAcquisitonRetailCleanPrice', u'MMRCurrentAuctionAveragePrice', u'MMRCurrentAuctionCleanPrice',\n",
      "             u'MMRCurrentRetailAveragePrice', u'MMRCurrentRetailCleanPrice', u'VehBCost', u'WarrantyCost']\n",
      "    dummy_cols = ['Make','Transmission','Size','AUCGUART', 'Auction', 'Color']\n",
      "    target_col = ['IsBadBuy']\n",
      "    if oos:\n",
      "        target_col = ['RefId']\n",
      "\n",
      "    # create the data frame used for analysis\n",
      "    ldf = lem[target_col + cont_cols + dummy_cols]\n",
      "\n",
      "    # correct the null values in AUCGART\n",
      "    ldf.AUCGUART = ldf.AUCGUART.map({'GREEN':1 ,'RED':0, nan: 0})\n",
      "    \n",
      "    # make Transmission a dummy variable Auto = 0, Manual = 1\n",
      "    ldf.Transmission = ldf.Transmission.map({'AUTO':0, 'MANUAL':1, 'Manual':1})\n",
      "\n",
      "    #drop rows with missing data\n",
      "    ldf = ldf.dropna()\n",
      "\n",
      "    # create a number of dummy variables based on categorical features and drop the original columns\n",
      "    ldf = ldf.join(pd.get_dummies(ldf['Make'], prefix='make'))\n",
      "    ldf = ldf.join(pd.get_dummies(ldf['Size'], prefix='size'))\n",
      "    ldf = ldf.join(pd.get_dummies(ldf['Auction'], prefix='auction'))\n",
      "    ldf = ldf.join(pd.get_dummies(ldf['Color'], prefix='color'))\n",
      "    ldf = ldf.drop(labels=['Make', 'Size', 'Auction', 'Color'], axis=1)\n",
      "                       \n",
      "    return ldf        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Model Functions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Benchmark Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def benchmark_model(X, y):\n",
      "    \"\"\"\n",
      "    create the benchmark model and returns it\n",
      "    \"\"\"\n",
      "    best_score = -1\n",
      "    for depth in range(1, 10):\n",
      "        scores = cross_val_score(tree.DecisionTreeClassifier(max_depth=depth, random_state=1234),\n",
      "                                X,\n",
      "                                y,\n",
      "                                scoring='roc_auc',\n",
      "                                cv=5)\n",
      "        if scores.mean() > best_score:\n",
      "            best_depth = depth\n",
      "            best_score = scores.mean()\n",
      "    \n",
      "    # Create a classifier and return\n",
      "    return tree.DecisionTreeClassifier(max_depth=best_depth, random_state=1234).fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Qasim's Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import feature_selection as f_select\n",
      "\n",
      "def qasim_test(classifier, X, y, ts= 0.3, cv = 6, print_out = False):\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.cross_validation import train_test_split\n",
      "    from sklearn import metrics\n",
      "    from sklearn.cross_validation import cross_val_score\n",
      "    from sklearn.naive_bayes import BernoulliNB\n",
      "    '''\n",
      "    significant_features = []\n",
      "    for feature in X:\n",
      "        pval = f_select.f_classif(X,y)\n",
      "        if pval[1][0] < 0.05:\n",
      "            significant_features.append(feature)\n",
      "    X = significant_features\n",
      "    '''\n",
      "    ROC = []\n",
      "    CV_Accuracy = []\n",
      "    \n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state= 1234)\n",
      "    model = classifier\n",
      "    model.fit(X_train, y_train)\n",
      "    predicted = model.predict(X_test.astype(int))\n",
      "    probs = model.predict_proba(X_test.astype(int))\n",
      "    scores = cross_val_score(classifier, X.astype(int), y, cv=cv)\n",
      "    ROC.append(metrics.roc_auc_score(y_test, probs[:, 1]))\n",
      "    CV_Accuracy.append(scores.mean())\n",
      "    \n",
      "    '''\n",
      "    if probs >=  .85:\n",
      "        X = X + predicted\n",
      "        y = calldf['Complaint Type']\n",
      "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state= 1234)\n",
      "        model2 = classifier2\n",
      "        model2.fit(X_train, y_train)\n",
      "        predicted = model2.predict(X_test.astype(int))\n",
      "        probs = model2.predict_proba(X_test.astype(int))\n",
      "        scores = cross_val_score(classifier2, X.astype(int), y, cv=cv)\n",
      "        ROC.append(metrics.roc_auc_score(y_test, probs[:, 1]))\n",
      "        CV_Accuracy.append(scores.mean())    \n",
      "        \n",
      "        print predicted\n",
      "     '''   \n",
      "        \n",
      "    \n",
      "    if print_out:\n",
      "        print 'Test Type', classifier\n",
      "        print '==========================================='\n",
      "        print 'Accuracy Percentage'\n",
      "        print metrics.accuracy_score(y_test, predicted)\n",
      "        print\n",
      "        print 'ROC score'\n",
      "        print metrics.roc_auc_score(y_test, probs[:, 1])\n",
      "        print\n",
      "        print 'Confusion Matrix'\n",
      "        print metrics.confusion_matrix(y_test, predicted)\n",
      "        print\n",
      "        print 'Classification Report'  \n",
      "        print metrics.classification_report(y_test, predicted)\n",
      "        print\n",
      "        print 'Cross Validation Score'\n",
      "        print scores.mean()\n",
      "        print\n",
      "    \n",
      "    return model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#############################\n",
      "# Example for the Benchmark #\n",
      "#############################\n",
      "print 'Benchmark Example'\n",
      "print '*****************'\n",
      "# create the benchmark data\n",
      "bldf = benchmark_lemons(lemons)\n",
      "y = bldf.IsBadBuy\n",
      "X = bldf.drop(labels=['IsBadBuy'], axis=1)\n",
      "\n",
      "# create the benchmark model and score\n",
      "bclf = benchmark_model(X, y)\n",
      "bench_score = metrics.roc_auc_score(y, bclf.predict(X))\n",
      "\n",
      "# test the benchmark model\n",
      "test_model(X, y, bclf)\n",
      "\n",
      "# create the submission\n",
      "print\n",
      "create_submission(X=benchmark_lemons(lemons_oos, oos=True), \n",
      "                  clf=bclf,\n",
      "                  name=\"benchmark_submit.csv\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Benchmark Example\n",
        "*****************\n",
        "TEST MODEL AUC: 0.5043"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "======================\n",
        "benchmark: 0.5000 (True)\n",
        "stratified: 0.5019 (True)\n",
        "most_frequent: 0.5000 (True)\n",
        "uniform: 0.5028 (True)\n",
        "\n",
        "Test file 'benchmark_submit.csv' successfully created."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### First Effort at Data Cleaning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###########################\n",
      "# Example with our Data A #\n",
      "###########################\n",
      "print\n",
      "print 'Sample Example'\n",
      "print '**************'\n",
      "# create a clean lemon data set for a new model\n",
      "ldf = clean_lemons_a(lemons)\n",
      "y = ldf.IsBadBuy\n",
      "X = ldf.drop(labels=['IsBadBuy'], axis=1)\n",
      "\n",
      "# use the benchmark model technique on our new data set\n",
      "clf = tree.DecisionTreeClassifier(max_depth=7, random_state=2345).fit(X, y)\n",
      "\n",
      "# show the basic metrics\n",
      "basic_metrics(y, clf.predict(X))\n",
      "print\n",
      "\n",
      "# test it\n",
      "test_model(X, y, clf, bench_score)\n",
      "\n",
      "# export it as a submission\n",
      "print\n",
      "create_submission(clean_lemons_a(lemons_oos, oos=True), clf, 'test_submit_a.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Sample Example\n",
        "**************\n",
        "Accuracy: 0.8787"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ROC AUC:  0.5112\n",
        "Confusion Matrix:\n",
        "[[44549    51]\n",
        " [ 6117   148]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TEST MODEL AUC: 0.5112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "======================\n",
        "benchmark: 0.5043 (True)\n",
        "stratified: 0.5020 (True)\n",
        "most_frequent: 0.5000 (True)\n",
        "uniform: 0.5034 (True)\n",
        "\n",
        "Test file 'test_submit_a.csv' successfully created."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###########################\n",
      "# Example Qasim on Data A #\n",
      "###########################\n",
      "\n",
      "# run the regressions on the clean_lemons_a data\n",
      "types = [LogisticRegression(),BernoulliNB(),DecisionTreeClassifier(),RandomForestClassifier()]\n",
      "clf_list = []\n",
      "for i in types:\n",
      "    clf_list.append(qasim_test(i, X, y))\n",
      "\n",
      "# test each model\n",
      "for i in range(0, len(clf_list)):\n",
      "    print types[i]\n",
      "    test_model(X, y, clf_list[i], bench_score)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
        "TEST MODEL AUC: 0.5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "======================\n",
        "benchmark: 0.5043 (False)\n",
        "stratified: 0.4996 (True)\n",
        "most_frequent: 0.5000 (False)\n",
        "uniform: 0.5017 (False)\n",
        "\n",
        "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "TEST MODEL AUC: 0.5007"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "======================\n",
        "benchmark: 0.5043 (False)\n",
        "stratified: 0.4984 (True)\n",
        "most_frequent: 0.5000 (True)\n",
        "uniform: 0.4975 (True)\n",
        "\n",
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best')\n",
        "TEST MODEL AUC: 0.8683"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "======================\n",
        "benchmark: 0.5043 (True)\n",
        "stratified: 0.5022 (True)\n",
        "most_frequent: 0.5000 (True)\n",
        "uniform: 0.5009 (True)\n",
        "\n",
        "RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0)\n",
        "TEST MODEL AUC: 0.8047"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "======================\n",
        "benchmark: 0.5043 (True)\n",
        "stratified: 0.5014 (True)\n",
        "most_frequent: 0.5000 (True)\n",
        "uniform: 0.5003 (True)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Add Color Dummies to Data A"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### DATA B ###\n",
      "# create a clean lemon data set, now with color\n",
      "ldf_b = clean_lemons_b(lemons)\n",
      "y_b = ldf_b.IsBadBuy\n",
      "X_b = ldf_b.drop(labels=['IsBadBuy'], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###########################\n",
      "# Example with our Data B #\n",
      "###########################\n",
      "\n",
      "# use the benchmark model technique on our new data set\n",
      "clf_b = tree.DecisionTreeClassifier(max_depth=7, random_state=2345).fit(X_b, y_b)\n",
      "\n",
      "# show the basic metrics\n",
      "basic_metrics(y_b, clf_b.predict(X_b))\n",
      "print\n",
      "\n",
      "# test it\n",
      "test_model(X_b, y_b, clf_b, bench_score)\n",
      "\n",
      "# export it as a submission\n",
      "print\n",
      "create_submission(clean_lemons_b(lemons_oos, oos=True), clf_b, 'test_submit_b1.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.8788\n",
        "ROC AUC:  0.5115\n",
        "Confusion Matrix:\n",
        "[[44549    51]\n",
        " [ 6114   151]]\n",
        "\n",
        "TEST MODEL AUC: 0.5115"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "======================\n",
        "benchmark: 0.5043 (True)\n",
        "stratified: 0.4969 (True)\n",
        "most_frequent: 0.5000 (True)\n",
        "uniform: 0.4996 (True)\n",
        "\n",
        "Test file 'test_submit_b.csv' successfully created."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "########################\n",
      "# Overfit on a small group and see contribute results\n",
      "#######################\n",
      "# run the regressions on the clean_lemons_a data\n",
      "over_fitted = DecisionTreeClassifier()\n",
      "qasim_test(over_fitted, X_b, y_b)\n",
      "\n",
      "test_model(X_b, y_b, over_fitted, bench_score)\n",
      "\n",
      "create_submission(clean_lemons_b(lemons_oos, oos=True), over_fitted, 'test_submit_b2.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TEST MODEL AUC: 0.8656\n",
        "======================\n",
        "benchmark: 0.5043 (True)\n",
        "stratified: 0.5022 (True)\n",
        "most_frequent: 0.5000 (True)\n",
        "uniform: 0.4997 (True)\n",
        "Test file 'test_submit_b2.csv' successfully created."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}